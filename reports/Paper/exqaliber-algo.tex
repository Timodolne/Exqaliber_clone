
\section{Algorithm}
As discussed in \ref{sec::intro} the algorithm is made up of two parts, the sampling algorithms which provides a dynamic sampling scheme to reach the desired accuracy and the post-processing algorithm which generate the final estimate of $\theta$ in an offline fashion. The reason for this two stage approach is that maintaining a true representation of the current state and uncertainty during the sampling scheme is too computationally intensive so we utilise a low dimensional approximation. In most cases provides a high quality estimate which enables us to carry out an online optimisation of sample number and depth. Once all the samples are obtain rather than return the approximation, which does not have the full predictive power, we re-estimate $\theta$ utilising the full data set that we have obtained during the sampling. This means the errors introduced by the approximation procedure do not effect the final estimate.

In this paper the novel work is in the sampling procedure and the associated modelling. For the post-processing we use the maximum likelihood estimate (MLE) originally described in \cite{MLE}. For this reason here we concentrate on the sampling algorithm and include a short introduction to the MLE at the end of this section.

\subsection{Sampling algorithm}
Here we describe the sampling algorithm and associated theoretical underpinning initially in the noiseless case and then introduce the adjustment required to deal with the introduction of depolarising noise. The fundamental idea is that we maintain a current state representing our belief about the value of the parameter of interest, $\mu$, and the current level of uncertainty, $\sigma^2$. Then at each step we decide if enough samples have been obtained, i.e. $\sigma^2$ is small enough for the required accuracy, or to generate another sample.

If another sample is generated we need to select the depth at which to sampling circuit is executed and using the result update the state, i.e. our belief. In both cases we use a Bayesian framework to do this, see \ref{sec::bayes}. Within this framework, the uncertainty $\sigma^2$ is an estimate of the posterior variance for $\theta$ and $\mu$, the current belief of $\theta$, is an estimate of the posterior mean. At each step the next sampling depth is selected to minimise the expected posterior variance given the estimate of the current belief, i.e. the depth which in expectation moves us closest to terminating the sampling. After executing the sampling circuit, the state is updated by calculating the posterior mean and variance. This is where the main approximation is used, unfortunately maintaining and manipulating  an estimate of the true posterior distribution is computationally intensive so after every step we approximate the posterior distribution by a normal distribution. This is then used as the prior distribution for the next step, for more details see Section \ref{sec::bayes}.

Having described the algorithm we now provide a pseudocode specification of the algorithm, see \ref{alg::sample}. There are couple of points to note from this pseudocode, firstly the procedure `Sample' executes the quantum circuit shown in figure \ref{fig::circuit} where $d$ is the number of times the Grover operator is applied. The measurement of 0 or 1 is then stored as $x$. Secondly, as described in Section \ref{sec::post} we post process the obtained samples to produce the final estimate of $\theta$ so `Store' stores the obtained sample with the associated depth for later use.  Thirdly, there are three closed form mathematical functions, $g_1,g_2,h$, used within the pseudocode defined in Section \ref{sec::postprop}. Note that the only change to this sampling algorithm when considering the depolarizing noise is to $g_1,g_2,h$ otherwise the algorithm stays the same.

\begin{algorithm}
	\caption{Pseudocode for sampling algorithm }\label{alg::sample}
	\begin{algorithmic}
		\Require $(\mu, \sigma^2), \epsilon, \alpha$
		\While{$\sigma > \Phi^{-1}(1-\alpha)\epsilon$} \Comment{$\Phi$ is the normal distribution function}
		\State $d \gets \argmax_{d>0}(h(\mu,\sigma^2,d))$ \Comment{See equation \ref{eqn::varfactor} }
		\State $x\gets \Call{Sample}{$d$}$\Comment{Execute circuit depth at $d$}
		\State $\mu \gets g_1(\mu,\sigma^2,x,d)$
		\State $\sigma^2 \gets g_2(\mu,\sigma^2,x,d)$ \Comment{Update state, see equations \ref{eqn::mean} and \ref{eqn::var}}
		\State \Call{Store}{$(x,d)$} \Comment{Store observation and sample depth for post-processing}
		\EndWhile
	\end{algorithmic}
\end{algorithm}


\subsubsection{Bayesian framework}\label{sec::bayes}
Through the design of this algorithm we have embraced a Bayesian approach \cite{} and used this to generate a sequential sampling algorithm. Note, an important difference from the standard sequential algorithms is that normally we are only interested in deciding when to stop sampling but here we also have control of the sampling distribution.

The fundamental feature of this approach is that at all stages we maintain a belief about our current knowledge of the unknown parameter, $\theta$, through a probability distribution, $\pi(\theta)$. This belief is updated after each sample using Bayes formula \cite{} and the current belief is used to make the decision of whether to  stop or generate a new sample and in that case what depth to sample at.

Unfortunately, keeping the full probability distribution is computationally infeasible so instead we approximate our belief by a normal distribution, i.e. $\pi(\theta)~N(\mu,\sigma^2)$, which is fully described by two parameters the mean, $\mu$, and the variance, $\sigma^2$. Therefore at each stage we approximate the posterior distribution, which becomes the prior when considering the next sample, by a normal distribution with the same variance and mean as the one step posterior distribution. Note that this mean and variance will not necessarily match the posterior mean and variance if all the obtained samples were considered due to the repeated approximation.

The normal distribution is a sensible approximation since under suitable conditions, see \cite{}, in the limit of a large number of samples the posterior limits to a normal distribution. Further to this using the normal distribution as our prior belief, we are able to use Bayes theorem to produce analytic expressions for a number of key posterior parameters including the posterior mean and variance and expected posterior variance, see the next section. This is important because we could obtain estimates of these through sampling schemes \cite{} but as we will discuss in Section \ref{sec::model} this leads to a systematic bias.

Though using a normal approximation will generally provide a good ongoing estimate but there are approximation errors which can be substantial in nature. For some experimental examples of this see Section\ref{}. This is the reason that we carry out a two stage algorithm and post process the samples to provide the final estimate for the parameter of interest, Section \ref{sec::postprop}. Two relevant examples of approximation errors are:
\begin{itemize}
	\item The parameter we are interested $\theta$ is best thought as angle, i.e. $0$ and $2\pi$ should be identified with each other. This means that we have to be especially careful near boundary points, for example 0.
	\item We are sampling from a Bernoulli distribution where the probability of a 1 is determined by the unknown parameter $\theta$ and the circuit depth. Unfortunately, as discussed in Section \ref{} the exact mapping to probability has a degeneracy when consider depths greater than 1, i.e. there are multiple values of $\theta$ which give the same value of a 1. If care is not taken this can lead to a multimodal posterior distribution.
\end{itemize}

\subsubsection{Posterior properties}\label{sec::postprop}
Two keys steps in the algorithm is calculating for a single sample the expected posterior variance, $Var(\theta)$ which is used to decide on the next sample depth and the posterior mean and variance, $\mathbb{E}(\theta|X=x)$ and $Var(\theta|X=x)$. As we have already seen in the pseudocode these play a significant role in our algorithm. We are able to calculate all three and obtained a closed form expression under the assumption that the prior follows a normal distribution with known mean and variance. Our derivation follows closely the work described in \cite{} which considers a class of likelihoods of the same form as we see in this work. Details of the derivation can be found in Appendix \ref{app::noiseless} and here we only include the final formulas for use with the algorithm.

\begin{align}
Var(\theta) &= TBA \label{eqn::postvac}\\
 \mathbb{E}(\theta|X=x)= g_1(\mu,\sigma^2,x,d) &= TBA \label{eqn::mean}\\
 Var(\theta|X=x)=g_2(\mu,\sigma^2,x,d) &= TBA \label{eqn::var}
\end{align}


\subsubsection{Selecting next sample depth}

The key advantage of the approach given in this work over the previous work, \cite{}, is the dynamic sampling scheme which utilises the current belief to minimise the work to obtain an estimate. To do this we look to maximise the information gain per sample or equivalently maximise the reduction in variance of our belief from each sample. The decision variable throughout is circuit depth for the next sample, i.e. the number applications of the Grover operators.

To do this we utilise the expected reduction in variance, see equation \ref{eqn::postvar}. Given our current state , $(\mu,\sigma^2)$, we select the depth that minimises the equation \ref{eqn::postvac}, note the optimisation is over integers. Rather than directly minimise this instead we look to maximise the variance reduction factor,
\begin{equation} h(\mu,\sigma^2,d) =?? \label{eqn::varfactor}\end{equation}
which is equivalent to minimising the expected variance. See figure \ref{} for an example of this function which is typical and integer points for which we are maximising over. There are a couple of features to note:
\begin{itemize}
	\item The various reduction function is bounded $...$ which is maximised by $...$. This reflects the scaling seen in the fixed strategy and in a number of works relating to quantum phase estimation \cite{}.
	\item The true maximum of integers might be a distance from the maximum of the bounding function due to the sinusoidal nature. Importantly it is not necessarily either integer value which neighbours the bounding maximum.
	\item The issue with selecting the bounding maximum is partially observed in this work, \cite{}. The driving factor is the gradient of the probability function for the Bernoulli distribution, near probability 0 and 1 this is small but nearer 1/2 this maximised which improves ability discriminate different values of $\theta$. This is something that can be clearly be observed in our experiments, see Section \ref{sec::?} and figure \ref{fig::?}.
\end{itemize}

\subsubsection{Adjustments for noise}
When we incorporate decoherence noise, we incorporate an extra parameter $\lambda$ which represents the noise level of the quantum computer. We assume in this work that the value of this parameter has been previously ascertained through other means. For reference, we repeat here the probability of measuring a value 1 as a function of $\theta$ and $\lambda$,
$$\mathbb{P}(X=1| \theta, \lambda)=To be added to be consistent.$$
For more further details of this see Section \ref{sec::?}.

As mentioned previously the basics sampling algorithm does not change, only the functions $h, g_1, g_2$ change to accommodate the change in the measurement probability. These become
\begin{align}
	h'(\mu,\sigma^2,d) &= TBA \label{eqn::npostvac}\\
	g'_1(\mu,\sigma^2,x,d) &= TBA \label{eqn::nmean}\\
	g'_2(\mu,\sigma^2,x,d) &= TBA \label{eqn::nvar}
\end{align}
For the derivation see Appendix \ref{app::noise}.


\subsubsection{Modelling the sampling scheme}\label{sec::model}

It is useful to note that we can model the algorithm as a Markov process on $\mathbb{R}^2$ with the state as $(\mu,\sigma^2)$. We can see this from equations \ref{eqn::mean} and \ref{eqn::var}, since the state update only depends on the current state, the depths which is again only a function of the current state and the random sample. This means that it obeys the Markov property, giving use a Markov process. If we look closely at equation \ref{eqn::var} we note that the update is multiplicative in nature. By taking the log of the variance, $\nu=\log(\sigma^2)$, we change this to additive process which provides us with a number of insights.

Firstly, if we consider the expected decrease in $\nu$ by taking the log of equation \ref{eqn::postvac} at the optimal depth we find this is $o(1)$, see Appendix \ref{app::decrease} through out. This means that the number of circuit executions to reach an accuracy of order $\epsilon$ will be $-\log(\epsilon)$ and $\nu$ decreases linearly in the number of iterations. This is something we observe in the experimentations, see Section \ref{}.

Secondly, by consider $\nu$ we can see why we need to calculate analytically the change in variance rather than use a sampling scheme. It is well known that the sample variance is an unbiased estimator of the true variance, i.e. $\mathbb{E}(S^2)=\sigma^2$ where $S^2$ is the sample variance, which would indicate that we can use a sample from the posterior distribution to estimate the variance. Unfortunately, this is not true as the $\log(S^2)$ is not an unbiased estimator of $\log(\sigma^2)$, we have $$ \mathbb{E}(\log(S^2)) = \mathbb{E}(\log(\sigma^2)) +\kappa, $$ where $\kappa>0$. For details, see Appendix \ref{app::logvar}. This means that if the sample variance is used to estimate the variance of the posterior we have a systematic drift which means the uncertainty is underestimated. This is not so problematic in the noiseless case as all steps of $\nu$ are order 1 but in the case with noise the step size decays with $\nu$ so the systematic drift dominates leading to under sampling.



{\color{red} {\bf Note:} Can we prove something about optimality of greedy strategy given the maximum reduction factor and bounds on the value function? (we know bounded between inverse square and inverse)}



\subsection{Post processing}\label{sec::post}
As mentioned previously the use of the normal approximation for the posterior distribution can lead to a drift in the estimate of $\theta$. For reason we carry out a post-processing step which utilises all the obtained samples by estimating $\theta$ by the maximum likelihood estimate (MLE) as described in \cite{}. There are two important points to note about this:
\begin{enumerate}\item The use of the MLE in this case is not turning our back on the Bayesian approach in the rest of the paper. Here assuming the prior distribution is uniformly distributed over the interval $[0,\pi]$, an uninformative prior,  the mode of the posterior distribution, a standard point estimate in Bayesian statistics \cite{}, agrees with the MLE.
\item This approach is easily adapted to the case of decoherence noise. The likelihood is changed to incorporate the extra factor in obvious fashion.
\end{enumerate}

ADD ALG AS A REMINDER

\newpage
